# Size of all main train corpora (De->En, En->Uk)
DATA_TRAIN_SIZE=200000

# Size of augmentation corpus (De->Ru)
DATA_AUG_SIZE=0

# Size of validation corpus (De->Uk)
DATA_VAL_SIZE=5000

# Size of test corpora (De->Uk)
DATA_TEST_SIZE=10000

# Size of the tokenizer vocabulary
VOCAB_SIZE=16000

# Reserved service token
RESERVED_TOKENS=["[PAD]", "[UNK]", "[START]", "[END]", "<2en>", "<2uk>"]

# Source languages token file path
SRC_FILE=vocabs/src_vocab.txt

# Target languages token file path
TGT_FILE=vocabs/tgt_vocab.txt

# Max number of tokens in sentences
MAX_TOKENS=128

# Size of batches
BATCH_SIZE=256

# Transformer parameters
NUM_LAYERS=4
D_MODEL=128
DFF=512
NUM_HEADS=8
DROPOUT_RATE=0.1

# Checkpoint folder path
CHECKPOINT_PATH="checkpoints/checkpoints_base"

# Number of epochs to train
EPOCHS=10

# Export and import directories
EXPORT_DIR=trained_models/augmented
IMPORT_DIR=trained_models/Updated
TEST_SENTENCES=test/test_sentences.txt
TEMPLATES_FOLDER=templates
TEMPLATE_TRANSLATION=translations.htm

# Sentence examples
SOURCE_SENTENCE=<2uk> Er wurde krank, weil er im Winter ohne Hut ging
GROUND_TRUTH=Він захворів, тому що взимку ходив без шапки
